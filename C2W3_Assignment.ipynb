{
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Week 3: Transfer Learning\n\nWelcome to this assignment! This week, you are going to use a technique called `Transfer Learning` in which you utilize an already trained network to help you solve a similar problem to the one it was originally trained to solve.\n\nLet's get started!",
      "metadata": {
        "id": "f8cj-HBNoEZy"
      },
      "id": "0d87e112-1b1c-48ac-93e9-c868bd612d59"
    },
    {
      "cell_type": "code",
      "source": "import os\nimport zipfile\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import img_to_array, load_img",
      "metadata": {
        "id": "lbFmQdsZs5eW",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "38e0a585-12eb-4cfb-86a2-693bba1557bc"
    },
    {
      "cell_type": "markdown",
      "source": "## Dataset\n\nFor this assignment, you will use the `Horse or Human dataset`, which contains images of horses and humans. \n\nDownload the `training` and `validation` sets by running the cell below:",
      "metadata": {
        "id": "RPvtLK1GyUWr"
      },
      "id": "ad07011d-b0e1-41f2-b725-dc77d0160afd"
    },
    {
      "cell_type": "code",
      "source": "# Get the Horse or Human training dataset\n!wget -q -P /content/ https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n\n# Get the Horse or Human validation dataset\n!wget -q -P /content/ https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip\n\ntest_local_zip = './horse-or-human.zip'\nzip_ref = zipfile.ZipFile(test_local_zip, 'r')\nzip_ref.extractall('/tmp/training')\n\nval_local_zip = './validation-horse-or-human.zip'\nzip_ref = zipfile.ZipFile(val_local_zip, 'r')\nzip_ref.extractall('/tmp/validation')\n\nzip_ref.close()",
      "metadata": {
        "id": "dIeTNcPEo79J",
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "84703cd9-0c9b-4884-9db8-42cd3e6b6b14"
    },
    {
      "cell_type": "markdown",
      "source": "This dataset already has an structure that is compatible with Keras' `flow_from_directory` so you don't need to move the images into subdirectories as you did in the previous assignments. However, it is still a good idea to save the paths of the images so you can use them later on:",
      "metadata": {
        "id": "x4OMDxYS6tmv"
      },
      "id": "e4af8534-f319-464b-92b3-362eff3cef83"
    },
    {
      "cell_type": "code",
      "source": "# Define the training and validation base directories\ntrain_dir = '/tmp/training'\nvalidation_dir = '/tmp/validation'\n\n# Directory with training horse pictures\ntrain_horses_dir = os.path.join(train_dir, 'horses')\n# Directory with training humans pictures\ntrain_humans_dir = os.path.join(train_dir, 'humans')\n# Directory with validation horse pictures\nvalidation_horses_dir = os.path.join(validation_dir, 'horses')\n# Directory with validation human pictures\nvalidation_humans_dir = os.path.join(validation_dir, 'humans')\n\n# Check the number of images for each class and set\nprint(f\"There are {len(os.listdir(train_horses_dir))} images of horses for training.\\n\")\nprint(f\"There are {len(os.listdir(train_humans_dir))} images of humans for training.\\n\")\nprint(f\"There are {len(os.listdir(validation_horses_dir))} images of horses for validation.\\n\")\nprint(f\"There are {len(os.listdir(validation_humans_dir))} images of humans for validation.\\n\")",
      "metadata": {
        "id": "lHRrmo5CpEw_",
        "lines_to_next_cell": 2,
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "bde65dc0-b216-426d-becd-1beb8acb3855"
    },
    {
      "cell_type": "markdown",
      "source": "Now take a look at a sample image of each one of the classes:",
      "metadata": {
        "id": "1G5hXBB57c78"
      },
      "id": "600e192c-8f2b-4c04-91e8-3fe19368ef5e"
    },
    {
      "cell_type": "code",
      "source": "print(\"Sample horse image:\")\nplt.imshow(load_img(f\"{os.path.join(train_horses_dir, os.listdir(train_horses_dir)[0])}\"))\nplt.show()\n\nprint(\"\\nSample human image:\")\nplt.imshow(load_img(f\"{os.path.join(train_humans_dir, os.listdir(train_humans_dir)[0])}\"))\nplt.show()",
      "metadata": {
        "id": "HgbMs7p0qSKr",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "68bd4090-2702-460c-b879-aea2c2817769"
    },
    {
      "cell_type": "markdown",
      "source": "`matplotlib` makes it easy to see that these images have a resolution of 300x300 and are colored, but you can double check this by using the code below:",
      "metadata": {
        "id": "LBnbnY0c8Zd0"
      },
      "id": "f197a6e2-f83f-4e9f-a5d3-dae2626a8697"
    },
    {
      "cell_type": "code",
      "source": "# Load the first example of a horse\nsample_image  = load_img(f\"{os.path.join(train_horses_dir, os.listdir(train_horses_dir)[0])}\")\n\n# Convert the image into its numpy array representation\nsample_array = img_to_array(sample_image)\n\nprint(f\"Each image has shape: {sample_array.shape}\")",
      "metadata": {
        "id": "4lIGjHC5pxua",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "d3be545c-d453-410c-9e9a-fad2beebebba"
    },
    {
      "cell_type": "markdown",
      "source": "As expected, the sample image has a resolution of 300x300 and the last dimension is used for each one of the RGB channels to represent color.",
      "metadata": {
        "id": "4fYwAYyd8zEm"
      },
      "id": "db645d74-eca8-473a-86af-9d25e4c46e61"
    },
    {
      "cell_type": "markdown",
      "source": "## Training and Validation Generators\n\nNow that you know the images you are dealing with, it is time for you to code the generators that will fed these images to your Network. For this, complete the `train_val_generators` function below:\n\n**Important Note:** The images have a resolution of 300x300 but the `flow_from_directory` method you will use allows you to set a target resolution. In this case, **set a `target_size` of (150, 150)**. This will heavily lower the number of trainable parameters in your final network, yielding much quicker training times without compromising the accuracy!",
      "metadata": {
        "id": "6HcE1TSqNRY2"
      },
      "id": "3e62fee7-088f-479a-94c3-f506d008b304"
    },
    {
      "cell_type": "code",
      "source": "# GRADED FUNCTION: train_val_generators\ndef train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n  \"\"\"\n  Creates the training and validation data generators\n  \n  Args:\n    TRAINING_DIR (string): directory path containing the training images\n    VALIDATION_DIR (string): directory path containing the testing/validation images\n    \n  Returns:\n    train_generator, validation_generator: tuple containing the generators\n  \"\"\"\n  ### START CODE HERE\n\n  # Instantiate the ImageDataGenerator class \n  # Don't forget to normalize pixel values and set arguments to augment the images \n  train_datagen = ImageDataGenerator(rescale=1/255.0,\n                                     rotation_range=45,\n                                     width_shift_range=0.2,\n                                     height_shift_range=0.2,\n                                     shear_range=0.2,\n                                     zoom_range=0.2,\n                                     horizontal_flip=True,\n                                     fill_mode='nearest'\n                                    )\n                                     \n                                     \n\n  # Pass in the appropriate arguments to the flow_from_directory method\n  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n                                                      batch_size=32, \n                                                      class_mode='binary',\n                                                      target_size=(150, 150))\n\n  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n  # Remember that validation data should not be augmented\n  validation_datagen = ImageDataGenerator(rescale=1/255.0)\n\n  # Pass in the appropriate arguments to the flow_from_directory method\n  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n                                                                batch_size=32, \n                                                                class_mode='binary',\n                                                                target_size=(150, 150))\n  ### END CODE HERE\n  return train_generator, validation_generator\n",
      "metadata": {
        "cellView": "code",
        "id": "AX5Q3NL_FXMT",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "6777e69a-85b0-4ab0-811f-cdbf969c56d6"
    },
    {
      "cell_type": "code",
      "source": "# Test your generators\ntrain_generator, validation_generator = train_val_generators(train_dir, validation_dir)",
      "metadata": {
        "id": "8FLUUqMKFwVR",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "58e8d7a0-cec8-473f-be2f-524bd29f6dc2"
    },
    {
      "cell_type": "markdown",
      "source": "**Expected Output:**\n```\nFound 1027 images belonging to 2 classes.\nFound 256 images belonging to 2 classes.\n```",
      "metadata": {
        "id": "TszKWhunQaj4"
      },
      "id": "efd25a86-4b77-4cd1-9ee2-5a2a85beaa17"
    },
    {
      "cell_type": "markdown",
      "source": "## Transfer learning - Create the pre-trained model\n\nDownload the `inception V3` weights into the `/tmp/` directory:",
      "metadata": {
        "id": "Izx51Ju1rXwd"
      },
      "id": "93f10dc5-03a4-40b1-a6a5-b977717a8e09"
    },
    {
      "cell_type": "code",
      "source": "# Download the inception v3 weights\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5",
      "metadata": {
        "id": "-lEzPAqxrPcU",
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "580a0797-5e46-4943-8981-ebbf44cd8ae8"
    },
    {
      "cell_type": "markdown",
      "source": "Now load the `InceptionV3` model and save the path to the weights you just downloaded:",
      "metadata": {
        "id": "_zlXNulm9USZ"
      },
      "id": "7891aa8b-ec89-410f-9a8d-ae4cffb85fc5"
    },
    {
      "cell_type": "code",
      "source": "# Import the inception model  \nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Create an instance of the inception model from the local pre-trained weights\nlocal_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'",
      "metadata": {
        "id": "zfmRpsMf7E3-",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "0bb47a6a-ac85-4eef-b7ca-307df29995d6"
    },
    {
      "cell_type": "markdown",
      "source": "Complete the `create_pre_trained_model` function below. You should specify the correct `input_shape` for the model (remember that you set a new resolution for the images instead of the native 300x300) and make all of the layers non-trainable:",
      "metadata": {
        "id": "ZPQb0PkT9_3w"
      },
      "id": "6a13357e-981a-4222-923d-ebf5b9b54c2c"
    },
    {
      "cell_type": "code",
      "source": "# GRADED FUNCTION: create_pre_trained_model\ndef create_pre_trained_model(local_weights_file):\n  \"\"\"\n  Initializes an InceptionV3 model.\n  \n  Args:\n    local_weights_file (string): path pointing to a pretrained weights H5 file\n    \n  Returns:\n    pre_trained_model: the initialized InceptionV3 model\n  \"\"\"\n  ### START CODE HERE\n  pre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n                                  include_top = False, \n                                  weights = None) \n\n  pre_trained_model.load_weights(local_weights_file)\n\n  # Make all the layers in the pre-trained model non-trainable\n  for layer in pre_trained_model.layers:\n    layer.trainable = False\n\n  ### END CODE HERE\n\n  return pre_trained_model\n  ",
      "metadata": {
        "cellView": "code",
        "id": "x2JnQ6m8r5oe",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "2e5fbc01-009d-40a0-8266-6d360ad38184"
    },
    {
      "cell_type": "markdown",
      "source": "Check that everything went well by comparing the last few rows of the model summary to the expected output:",
      "metadata": {
        "id": "phE00SCr-RCT"
      },
      "id": "3f6070e1-aaf5-4925-bcae-04c9f062da49"
    },
    {
      "cell_type": "code",
      "source": "pre_trained_model = create_pre_trained_model(local_weights_file)\n\n# Print the model summary\npre_trained_model.summary()",
      "metadata": {
        "id": "ve7eh9iztT4q",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "c3cc7352-38e9-40c4-b61a-77994318ce60"
    },
    {
      "cell_type": "markdown",
      "source": "**Expected Output:**\n```\nbatch_normalization_v1_281 (Bat (None, 3, 3, 192)    576         conv2d_281[0][0]                 \n__________________________________________________________________________________________________\nactivation_273 (Activation)     (None, 3, 3, 320)    0           batch_normalization_v1_273[0][0] \n__________________________________________________________________________________________________\nmixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_275[0][0]             \n                                                                activation_276[0][0]             \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 3, 3, 768)    0           activation_279[0][0]             \n                                                                activation_280[0][0]             \n__________________________________________________________________________________________________\nactivation_281 (Activation)     (None, 3, 3, 192)    0           batch_normalization_v1_281[0][0] \n__________________________________________________________________________________________________\nmixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_273[0][0]             \n                                                                mixed9_1[0][0]                   \n                                                                concatenate_5[0][0]              \n                                                                activation_281[0][0]             \n==================================================================================================\nTotal params: 21,802,784\nTrainable params: 0\nNon-trainable params: 21,802,784\n\n\n```",
      "metadata": {
        "id": "4cAY2gQytr0-"
      },
      "id": "2650dc02-c7ef-450e-8827-4cc545547b46"
    },
    {
      "cell_type": "markdown",
      "source": "To check that all the layers in the model were set to be non-trainable, you can also run the cell below:",
      "metadata": {
        "id": "MRHkV9jo-hkh"
      },
      "id": "ed5934bd-5b46-4694-8dc8-60d9690f7375"
    },
    {
      "cell_type": "code",
      "source": "total_params = pre_trained_model.count_params()\nnum_trainable_params = sum([w.shape.num_elements() for w in pre_trained_model.trainable_weights])\n\nprint(f\"There are {total_params:,} total parameters in this model.\")\nprint(f\"There are {num_trainable_params:,} trainable parameters in this model.\")",
      "metadata": {
        "id": "VASOaB8xDbhU",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "6d6ecdf0-d92e-4e28-af79-06e0d954dde3"
    },
    {
      "cell_type": "markdown",
      "source": "**Expected Output:**\n```\nThere are 21,802,784 total parameters in this model.\nThere are 0 trainable parameters in this model.\n```",
      "metadata": {
        "id": "mRioO7FH5a8I"
      },
      "id": "b7017591-d34f-44c3-858f-512f8af9aaf1"
    },
    {
      "cell_type": "markdown",
      "source": "## Creating callbacks for later\n\nYou have already worked with callbacks in the first course of this specialization so the callback to stop training once an accuracy of 99.9% is reached, is provided for you:",
      "metadata": {
        "id": "dFtwDyKj-4GR"
      },
      "id": "bdebddd9-546c-4ba5-9519-078af55515a2"
    },
    {
      "cell_type": "code",
      "source": "# Define a Callback class that stops training once accuracy reaches 99.9%\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>0.999):\n      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n      self.model.stop_training = True",
      "metadata": {
        "id": "SeVjZD2o7gWS",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "797a2581-6af2-4b58-b7ac-6b192308d4d9"
    },
    {
      "cell_type": "markdown",
      "source": "## Pipelining the pre-trained model with your own\n\nNow that the pre-trained model is ready, you need to \"glue\" it to your own model to solve the task at hand.\n\nFor this you will need the last output of the pre-trained model, since this will be the input for your own. Complete the `output_of_last_layer` function below.\n\n**Note:** For grading purposes use the `mixed7` layer as the last layer of the pre-trained model. However, after submitting feel free to come back here and play around with this.",
      "metadata": {
        "id": "lHZnFl-5_p3a"
      },
      "id": "bcf9d106-53e1-4f5e-baa9-a1389a84bcb8"
    },
    {
      "cell_type": "code",
      "source": "# GRADED FUNCTION: output_of_last_layer\ndef output_of_last_layer(pre_trained_model):\n  \"\"\"\n  Gets the last layer output of a model\n  \n  Args:\n    pre_trained_model (tf.keras Model): model to get the last layer output from\n    \n  Returns:\n    last_output: output of the model's last layer \n  \"\"\"\n  ### START CODE HERE\n  last_desired_layer = pre_trained_model.get_layer('mixed7')\n  print('last layer output shape: ', last_desired_layer.output_shape)\n  last_output = last_desired_layer.output\n  print('last layer output: ', last_output)\n  ### END CODE HERE\n\n  return last_output\n",
      "metadata": {
        "id": "CFsUlwdfs_wg",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "419986c0-644e-4df7-a973-5e011e94c30a"
    },
    {
      "cell_type": "markdown",
      "source": "Check that everything works as expected:",
      "metadata": {
        "id": "13AEzKG2A6_J"
      },
      "id": "2ec3b779-b71c-41a5-8f7d-98c843edf190"
    },
    {
      "cell_type": "code",
      "source": "last_output = output_of_last_layer(pre_trained_model)",
      "metadata": {
        "id": "zOJPUtMN6PHo",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "eb25be9a-1438-4caa-9692-9b8e253e0e72"
    },
    {
      "cell_type": "markdown",
      "source": "**Expected Output (if `mixed7` layer was used):**\n```\nlast layer output shape:  (None, 7, 7, 768)\nlast layer output:  KerasTensor(type_spec=TensorSpec(shape=(None, 7, 7, 768), dtype=tf.float32, name=None), name='mixed7/concat:0', description=\"created by layer 'mixed7'\")\n```",
      "metadata": {
        "id": "XqIWKZ_h7CuY"
      },
      "id": "fc6133e1-db4a-4a4b-9066-12bc37d3647d"
    },
    {
      "cell_type": "markdown",
      "source": "Now you will create the final model by adding some additional layers on top of the pre-trained model.\n\nComplete the `create_final_model` function below. You will need to use Tensorflow's [Functional API](https://www.tensorflow.org/guide/keras/functional) for this since the pretrained model has been created using it. \n\nLet's double check this first:",
      "metadata": {
        "id": "0Rp-J6JuwJTq"
      },
      "id": "ca2c2c41-82c5-4e1d-adb1-829dc30754b4"
    },
    {
      "cell_type": "code",
      "source": "# Print the type of the pre-trained model\nprint(f\"The pretrained model has type: {type(pre_trained_model)}\")",
      "metadata": {
        "id": "cKQknB4j7K9y",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "5cc91937-f4c8-460f-b3b3-40f0d1aa6aab"
    },
    {
      "cell_type": "markdown",
      "source": "To create the final model, you will use Keras' Model class by defining the appropriate inputs and outputs as described in the first way to instantiate a Model in the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n\nNote that you can get the input from any existing model by using its `input` attribute and by using the Funcional API you can use the last layer directly as output when creating the final model.",
      "metadata": {
        "id": "Kt7AU7jP7LW9"
      },
      "id": "4bd05477-324c-48a9-b9e1-bbc5e3e2faa3"
    },
    {
      "cell_type": "code",
      "source": "# GRADED FUNCTION: create_final_model\ndef create_final_model(pre_trained_model, last_output):\n  \"\"\"\n  Appends a custom model to a pre-trained model\n  \n  Args:\n    pre_trained_model (tf.keras Model): model that will accept the train/test inputs\n    last_output (tensor): last layer output of the pre-trained model\n    \n  Returns:\n    model: the combined model\n  \"\"\"\n  # Flatten the output layer to 1 dimension\n  x = layers.Flatten()(last_output)\n\n  ### START CODE HERE\n\n  # Add a fully connected layer with 1024 hidden units and ReLU activation\n  x = layers.Dense(1024, activation='relu')(x)\n  # Add a dropout rate of 0.2\n  x = layers.Dropout(0.2)(x)\n  # Add a final sigmoid layer for classification\n  x = layers.Dense(1, activation='sigmoid')(x)  \n\n  # Create the complete model by using the Model class\n  model = Model(inputs=pre_trained_model.input, outputs=x)\n\n  # Compile the model\n  model.compile(optimizer = RMSprop(learning_rate=0.0001), \n                loss = 'binary_crossentropy',\n                metrics = ['accuracy'])\n\n  ### END CODE HERE\n  \n  return model\n",
      "metadata": {
        "cellView": "code",
        "id": "BMXb913pbvFg",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "373e02b0-7141-4c64-86d0-e175a13e258a"
    },
    {
      "cell_type": "code",
      "source": "# Save your model in a variable\nmodel = create_final_model(pre_trained_model, last_output)\n\n# Inspect parameters\ntotal_params = model.count_params()\nnum_trainable_params = sum([w.shape.num_elements() for w in model.trainable_weights])\n\nprint(f\"There are {total_params:,} total parameters in this model.\")\nprint(f\"There are {num_trainable_params:,} trainable parameters in this model.\")",
      "metadata": {
        "id": "cL6ga5Z1783H",
        "tags": [
          "graded"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "id": "058a0d8e-e513-41a3-b4e8-3b67ffd0cbed"
    },
    {
      "cell_type": "markdown",
      "source": "**Expected Output:**\n```\nThere are 47,512,481 total parameters in this model.\nThere are 38,537,217 trainable parameters in this model.\n```",
      "metadata": {
        "id": "J4d3zlcQDrvm"
      },
      "id": "3a275ae3-83f6-4136-bba1-a5051ddd62df"
    },
    {
      "cell_type": "markdown",
      "source": "Wow, that is a lot of parameters!\n\nAfter submitting your assignment later, try re-running this notebook but use the original resolution of 300x300, you will be surprised to see how many more parameters are for that case.\n\nNow train the model:",
      "metadata": {
        "id": "_eqwHj5xEBZ7"
      },
      "id": "7e6e3d9e-3534-4157-ba58-8e72ecda2f66"
    },
    {
      "cell_type": "code",
      "source": "# Run this and see how many epochs it should take before the callback\n# fires, and stops training at 99.9% accuracy\n# (It should take a few epochs)\ncallbacks = myCallback()\nhistory = model.fit(train_generator,\n                    validation_data = validation_generator,\n                    epochs = 100,\n                    verbose = 2,\n                    callbacks=callbacks)",
      "metadata": {
        "id": "Blhq2MAUeyGA",
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "3381f79b-a471-450e-a940-e984fc137c68"
    },
    {
      "cell_type": "markdown",
      "source": "The training should have stopped after less than 10 epochs and it should have reached an accuracy over 99,9% (firing the callback). This happened so quickly because of the pre-trained model you used, which already contained information to classify humans from horses. Really cool!\n\nNow take a quick look at the training and validation accuracies for each epoch of training:",
      "metadata": {
        "id": "Y94djl4t0sK5"
      },
      "id": "f138cbb7-e18d-4b80-a802-cff11775a26d"
    },
    {
      "cell_type": "code",
      "source": "# Plot the training and validation accuracies for each epoch\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\nplt.show()",
      "metadata": {
        "id": "C2Fp6Se9rKuL",
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "5d45c20b-e30f-4bb7-80ea-6d64fc02ff91"
    },
    {
      "cell_type": "markdown",
      "source": "You will need to submit this notebook for grading. To download it, click on the `File` tab in the upper left corner of the screen then click on `Download` -> `Download .ipynb`. You can name it anything you want as long as it is a valid `.ipynb` (jupyter notebook) file.",
      "metadata": {
        "id": "g-4-4i9U1a0s"
      },
      "id": "819eef8f-81e0-481d-b29f-01a3642f0183"
    },
    {
      "cell_type": "markdown",
      "source": "**Congratulations on finishing this week's assignment!**\n\nYou have successfully implemented a convolutional neural network that leverages a pre-trained network to help you solve the problem of classifying humans from horses.\n\n**Keep it up!**",
      "metadata": {
        "id": "7w54-pbB1W9r"
      },
      "id": "f4f22ce2-af54-4653-a015-d3c5a5833034"
    }
  ]
}
